{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d4f2a5-17e0-4afe-8655-d679fba81853",
   "metadata": {},
   "source": [
    "# Module 4 - Table Self-query Q&A\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the \"Data Science 3.0\" image.\n",
    "</div>\n",
    "\n",
    "In this notebook we will walk through how to perform _\"self querying\"_ with table data wth tables present in documents. First we will be extracting the tables from a document using Amazon Textract using `AnalyzeDocument` API, generating the table data and then store the table data into a Vector DB in a very specific way, and then performing self-querying on the table data with a Anthropic Claude model via Amazon Bedrock and get precise answers from the model. We will be using open-source [ChromaDB](https://github.com/chroma-core/chroma) as our in-memory vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756f4b7-f2ff-4069-82cf-5adff4f5d610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "br = boto3.client('bedrock')\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(f\"SageMaker bucket is {data_bucket}, and SageMaker Execution Role is {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cbbbf6-56da-41dc-a7de-5496714fafc2",
   "metadata": {},
   "source": [
    "## Extract table data from the document using Amazon Textract\n",
    "---\n",
    "\n",
    "For this module, we will be using a sample bank statement document (`/sample-docs/bank_statement.jpg`) that contains tables data. We will use the `amazon-textract-textractor` library to perform the API call to `AnalyzeDocument` with `TABLE` feature and also read the table data with the Textract response parser. Once the tables are extracted we will parse out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215bee5-df6b-4c25-91db-330845870f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"./sample-docs/bank_statement.pdf\", width=600, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b1021-746e-45e0-b82c-f6378c1f1223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "from textractprettyprinter.t_pretty_print import Pretty_Print_Table_Format, Textract_Pretty_Print, get_string\n",
    "\n",
    "textract_json = call_textract(input_document=f\"s3://{data_bucket}/uploads/bank_statement.pdf\", features=[Textract_Features.TABLES])\n",
    "\n",
    "print(get_string(textract_json=textract_json,\n",
    "               table_format=Pretty_Print_Table_Format.tsv,\n",
    "               output_type=[Textract_Pretty_Print.TABLES]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b67a6-31ad-495a-bdcc-5be38b27182e",
   "metadata": {},
   "source": [
    "We notice that Textract has extracted two distinct tables from the document. In this walkthrough we will get the first table and perform Self-query on it using Langchain. There are two tables in this page, let's do Q&A on the first table. Note that we are going to use LangChain's `SelfQueryRetriever` which is helpful with Q&A with tables. As of this writing, FAISS is not supported for self-querying with LangChain, so we will use ChromaDB. For more information refer to the [Self-query LangChain documentation](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830eb1b8-bb94-4875-a55c-93da89f6b6c2",
   "metadata": {},
   "source": [
    "## Transform the extracted table data\n",
    "---\n",
    "\n",
    "Self query needs the table data to be formatted in a very specific way using LangChain's `Document` model. For example, here is what the structure looks like\n",
    "\n",
    "Table\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>year</th>\n",
    "        <th>director</th>\n",
    "        <th>rating</th>\n",
    "        <th>movie</th>\n",
    "        <th>actor</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2010</td>\n",
    "        <td>Christopher Nolan</td>\n",
    "        <td>8.2</td>\n",
    "        <td>Inception</td>\n",
    "        <td>Leo DiCaprio</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2006</td>\n",
    "        <td>Satoshi Kon</td>\n",
    "        <td>8.6</td>\n",
    "        <td>Paprika</td>\n",
    "        <td>Megumi Hayashibara</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "```python\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"2010, Christopher Nolan, 8.2, Inception, Leo DiCaprio\",\n",
    "        metadata={\"year\": 2010, \n",
    "                  \"director\": \"Christopher Nolan\", \n",
    "                  \"rating\": 8.2, \n",
    "                  \"movie\": \"Inception\", \n",
    "                  \"actor\": \"Leo DiCaprio\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"2006, Satoshi Kon, 8.6, Paprika, Megumi Hayashibara\",\n",
    "        metadata={\"year\": 2006, \n",
    "                  \"director\": \"Satoshi Kon\", \n",
    "                  \"rating\": 8.6,\n",
    "                  \"movie\": \"Paprika\",\n",
    "                  \"actor\": \"Megumi Hayashibara\"},\n",
    "    )\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Above, the table data rows represented by CSV string resides in the `page_content` key in the Document schema. The `metadata` section contains key-value pairs which are table header to cell value. The table may look something like below.\n",
    "\n",
    "We will transform the first table in the document using the same schema. We will do this by accessing the individual row/col data available in the Textract output using Textract response parser utility tool. Note that our table contains numbers and as such for self-query to work we need to convert numbers into int or float type appropriately as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467a437-0402-4ac3-b99b-318612e2dd5d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "from trp import Document as TDoc\n",
    "from langchain.schema import Document\n",
    "\n",
    "doc = TDoc(textract_json)\n",
    "rows = []\n",
    "\n",
    "def detect_type(s):\n",
    "    if type(s) == 'NoneType': \n",
    "        return s\n",
    "    elif not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.replace(',', '')\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(s)\n",
    "        except ValueError:\n",
    "            return s\n",
    "\n",
    "# Extract the first table data\n",
    "for page in doc.pages:\n",
    "    if page.tables:\n",
    "        for row in page.tables[0].rows:\n",
    "            cells = [detect_type(cell.text.strip()) for cell in row.cells]\n",
    "            rows.append(cells)\n",
    "\n",
    "headers = rows[0]\n",
    "headers = [ f\"Transaction_{x}\" for x in headers ]\n",
    "full_table = []\n",
    "\n",
    "for row in rows[1:]:\n",
    "    output = StringIO()\n",
    "    csv_writer = csv.writer(output)\n",
    "    csv_writer.writerow(row)\n",
    "    csv_string = output.getvalue()\n",
    "    row_meta = {headers[i].replace('($)','').strip(): detect_type(cell) for i, cell in enumerate(row)}\n",
    "    full_table.append(Document(page_content=csv_string.strip(), metadata=row_meta))\n",
    "\n",
    "full_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c1dcd-736f-4385-92b5-634f2e0629da",
   "metadata": {},
   "source": [
    "\n",
    "## Store the table in Vector DB\n",
    "---\n",
    "\n",
    "We will now store this into our vector database by first generating embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fdbde-5025-455d-a3dd-f051b3cb7727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Ensure that you have enabled amazon.titan-embed-text-v1 model in Amazon Bedrock console\n",
    "# embeddings = BedrockEmbeddings(client=bedrock,model_id=\"amazon.titan-embed-text-v1\")\n",
    "\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceEmbeddings(encode_kwargs=encode_kwargs)\n",
    "\n",
    "vector_db = Chroma.from_documents(documents=full_table, embedding=embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5dcb55-0a62-47d4-9f13-e97044547265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CAUTION: If you execute this cell then the ChromaDB collection created in previous step will be deleted\n",
    "\n",
    "# vector_db.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77646708-518b-49a6-9704-65b8b2183e9d",
   "metadata": {},
   "source": [
    "## Self Query Retriever with Amazon Bedrock and Anthropic Claude\n",
    "---\n",
    "\n",
    "We will now create a self-query retriever, much like the retriever we used in the _\"In-context QA\"_ notebook. However this time we will use some additional information to create the retriever in addition to the vector database. We created a special structure using the table data in the previous code cell (`full_table`), we will also need to define the table definition using LangChain's `AttributeInfo` model. This will help the LLM understand what each of the column/header actually means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3a974-28be-4507-b59c-3a0d4201b372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Transaction_Date\",\n",
    "        description=\"Date of the bank transaction\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Transaction_Description\",\n",
    "        description=\"Description of the bank transaction\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Transaction_Deposits\",\n",
    "        description=\"The dollar amount deposited into the bank account\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Transaction_Withdrawals\",\n",
    "        description=\"The dollar amount withdrawn from the bank account\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Transaction_Amount\",\n",
    "        description=\"The total dollar amount balance in the bank account\",\n",
    "        type=\"integer\",\n",
    "    )\n",
    "]\n",
    "document_content_description = \"A transaction in a bank statement\"\n",
    "\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=\"anthropic.claude-instant-v1\", model_kwargs={'temperature':0})\n",
    "# retriever = SelfQueryRetriever.from_llm(\n",
    "#     bedrock_llm, vector_db, document_content_description, metadata_field_info, verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d33607-3b3f-4cef-a95f-bf81390788a8",
   "metadata": {},
   "source": [
    "### Constructing the prompt\n",
    "\n",
    "In this step we will build the prompt that will help transform the user's question (query) to a `StructuredQuery`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b735360-dca4-42f7-be8e-16ec8999b619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import get_query_constructor_prompt\n",
    "\n",
    "prompt = get_query_constructor_prompt(\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12da307-4599-40a5-82dc-cdcc7d38381a",
   "metadata": {},
   "source": [
    "Let's print out the prompt to see what it looks like. Notice that it is a \"few shot\" prompt, which means our LLM is shown a few example of how to create a query, using a given a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2f342-7c47-4f55-bfbe-bad0737ff2e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(prompt.format(query=\"dummy question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787550a-ba45-4946-9761-dcd3211ae184",
   "metadata": {},
   "source": [
    "### Constructing the StructuredQuery using the JSON query\n",
    "\n",
    "In the previous state, we built a prompt that instructs the model how to generate a JSON format query using a given schema. In this section we will chain it together with an `output_parser` which can parse that JSON and create a `StructuredQuery`. This `StructuredQuery` will then be used to query our table data that we loaded into the vector database earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec712973-4222-4968-af52-3b951dba7132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import StructuredQueryOutputParser\n",
    "\n",
    "output_parser = StructuredQueryOutputParser.from_components()\n",
    "\n",
    "query_constructor = prompt | bedrock_llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb216f72-bedb-450d-85ed-0579999a0d10",
   "metadata": {},
   "source": [
    "Let's see what the model generates as a `StructuredQeuery` based on a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6b6e4-9fca-456e-89a3-5ebf99640937",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(query_constructor.invoke(\n",
    "    {\n",
    "        \"query\": \"What are the transactions with withdrawals greater than 300?\"\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b59f31-e8f0-4b7c-b6b6-8fdc07ea3519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(query_constructor.invoke(\n",
    "    {\n",
    "        \"query\": \"What are the transactions with balance greater than 5000?\"\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a4fad-f806-4d2c-9bfd-3c7539784b3c",
   "metadata": {},
   "source": [
    "### Querying the table data using the structured query\n",
    "\n",
    "In the previous step we got a `StructuredQuery` from our plain language question. The last step is to hook it up with the vector database so that this `StructuredQuery` can be executed and the relevant data can be extracted from the table. We will use a `SelfQueryRetriever` along with the ChromaDB instance. Notice that we only need to pass our question since `SelfQueryRetriever` will construct the `StructuredQuery` internally (i.e. the step we saw above, and then execute the query on our vector db that contains the table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48419a88-b5bf-4fb2-8e84-9cc2b8923bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "\n",
    "retriever = SelfQueryRetriever(\n",
    "    query_constructor=query_constructor,\n",
    "    vectorstore=vector_db,\n",
    "    structured_query_translator=ChromaTranslator()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a0c38-c254-42e0-b3cb-1912638b5486",
   "metadata": {},
   "source": [
    "Let's invoke the `SelfQueryRetriever` with our questions. We wrote a small function that will perform the self_query on the retriever and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7174bd-7119-4c90-801b-ce6a07f611a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def get_table_data(query, attempt=1, max_attempts=5):\n",
    "    try:\n",
    "        results = retriever.invoke(query)\n",
    "        for res in results:\n",
    "            print(res.page_content)\n",
    "    except Exception as e:\n",
    "        if attempt <= max_attempts:\n",
    "            # Calculate delay with exponential backoff\n",
    "            delay = min(60, (2 ** attempt) + random.uniform(0, 1))  # Delay in seconds, max 60s\n",
    "            # print(f\"Attempt {attempt} failed with error: {e}. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            get_table_data(query, attempt + 1, max_attempts)\n",
    "        else:\n",
    "            print(f\"All {max_attempts} attempts failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8e762-3bbe-4311-b028-accf9747260f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_table_data(\"What are the transactions with withdrawals greater than 300?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993edd34-f0c1-4052-a771-271b9db8a82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_table_data(\"What are the transactions with balance greater than 5000?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8ca0e-cba8-441e-be16-2ca270be36b5",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "---\n",
    "\n",
    "Let's delete the data from our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99921b4a-7764-4da9-8b61-441b3e1affe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2030e67-dd60-4671-805b-7b9731e74363",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "---\n",
    "\n",
    "In this module we saw how we can extract Tabular structures from a document using Amazon Textract. We then generated formatted the table data into a specific format, along with metadata information such as column names etc. Then we generated embeddings from the data and stored it in the vector database. We were able to generate Queries using plain language prompts and subsequently the queries were exectued to grab the relevant table information from the vector database. In the next module, we will see how we can perform more elaborate chat/conversational style Q&A with our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a8dca-f23d-462b-b823-bdf0e4d34f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
