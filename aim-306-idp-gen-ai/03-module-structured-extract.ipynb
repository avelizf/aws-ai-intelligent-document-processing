{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518e1152-932f-4a62-9e3f-e434b9440c1c",
   "metadata": {},
   "source": [
    "# Module 3 - Structured data extraction\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the \"Data Science 3.0\" image.\n",
    "</div>\n",
    "\n",
    "In this notebook we will walk through how to perform _\"templating, normalizations, and entity extractions\"_ from text in documents. We will be using a document and it's extracted text from our workflow in Module 1 where the text was extracted using Amazon Textract `AnalyzeDocument` API with `LAYOUT` feature, subsequently we will use and LLM and prompt engineering techniques to get the data in desired format.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You can ignore any WARNINGS during the `pip installs`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cb7b8-279e-498f-8d26-477f286f357a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d2070-7317-4c35-8e35-732273765b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "br = boto3.client('bedrock')\n",
    "s3 = boto3.client('s3')\n",
    "textract = boto3.client('textract')\n",
    "print(f\"SageMaker bucket is {data_bucket}, and SageMaker Execution Role is {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602444b-d48b-404f-96e0-3ece4bd9ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"anthropic.claude-instant-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3789c0-11da-4da8-a7a1-d0268399c74f",
   "metadata": {},
   "source": [
    "# Templating & Normalizations\n",
    "---\n",
    "\n",
    "The most common way to extract information out of documents is via key-value pairs. At times you may want the output from your document to be in a specific format so that it's much easier to consume in your downstream system. One way is to specify a template of the the output structure.\n",
    "\n",
    "In this notebook we will use a document that has Form components in it as well as some dense text that is in columnar section. We will use Amazon Textract's layout feature to read the document in the correct reading order. However our final goal is to get a specific set of information (entities) in a specific format so that we can easily consume the output later downstream.\n",
    "\n",
    "Let's take a look at the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0d8dd-4432-4359-8f10-ecf229669285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "input_document_path=f\"s3://{data_bucket}/output/discharge-summary/text_pages\"\n",
    "Image(filename='./sample-docs/discharge-summary.png',width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d66e7c-31de-47a9-b1d3-da30c6a2067d",
   "metadata": {},
   "source": [
    "We will try to extract the following information from the document in key-value pair format.\n",
    "\n",
    "- Doctor's name\n",
    "- Provider ID\n",
    "- Patient's name\n",
    "- Patient ID\n",
    "- Patient gender\n",
    "- Patient age\n",
    "- Admitted date\n",
    "- Discharge date\n",
    "- Discharged to\n",
    "- Drug allergies\n",
    "\n",
    "Since we already have extracted linearized text from the document in S3, we will read the document text from that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacadbf-f104-4aa0-bfc9-350e668da6ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from read_doc_from_s3 import read_document\n",
    "document = read_document(doc_path=input_document_path)\n",
    "full_text = document[0].strip()\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5387b17-e764-4a32-8514-1b5898cd49fa",
   "metadata": {},
   "source": [
    "Let's get the text extracted by the LAYOUT feature. We have written a small linearizer function that generates the text in the proper reading order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d5534-d116-4265-bddf-df6dfe8c2808",
   "metadata": {},
   "source": [
    "## Define the extraction template\n",
    "---\n",
    "Based on the fields we need to extract, we will define a template that will be used by the LLM to extract the entities. Let's start by creating a template with the first six values we want to extract from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267149d-fe4a-4aba-a09d-593cc392aabd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "output_template= {\n",
    "    \"doctor_name\":{ \"type\": \"string\", \"description\": \"The doctor or provider's full name\" },\n",
    "    \"provider_id\":{ \"type\": \"string\", \"description\": \"The doctor or provider's ID\" },\n",
    "    \"patient_name\":{ \"type\": \"string\", \"description\": \"The patient's full name\" },\n",
    "    \"patient_id\":{ \"type\": \"string\", \"description\": \"The patient's ID\" },\n",
    "    \"patient_gender\":{ \"type\": \"string\", \"description\": \"The patient's gender\" },\n",
    "    \"patient_age\":{ \"type\": \"number\",  \"description\": \"The patient's age\" }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130578e-e406-4843-b3be-169873535440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "You are a helpful assistant. Please extract the following details from the document and format the output as JSON using the keys. Skip any preamble text and generate the final answer.\n",
    "\n",
    "<details>\n",
    "{details}\n",
    "</details>\n",
    "\n",
    "<keys>\n",
    "{keys}\n",
    "</keys>\n",
    "\n",
    "<document>\n",
    "{doc_text}\n",
    "<document>\n",
    "\n",
    "<final_answer>\"\"\"\n",
    "\n",
    "\n",
    "details = \"\\n\".join([f\"{key}: {value['description']}\" for key, value in output_template.items()])\n",
    "keys = \"\\n\".join([f\"{key}\" for key, value in output_template.items()])\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"details\", \"keys\", \"doc_text\"])\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bedrock_llm)\n",
    "output = llm_chain.run({\"doc_text\": full_text, \"details\": details, \"keys\": keys})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9568479-71de-49f5-9d87-ed8d27674128",
   "metadata": {
    "tags": []
   },
   "source": [
    "## More Structure with LangChain Response Schemas\n",
    "---\n",
    "\n",
    "In the above example we were providing `keys` and `details` to the prompt template by creating strings from the extraction template. A better way is to use LangChain `ResponseSchema` to define the schema, and then use a `PydanticOutputParser` which will generate the format instruction for the LLM. To use `PydanticOutputParser` **we define the template using a small Python class instead of free-form JSON**. We can then use that format instruction with our prompt template and subsequently even use the output parser to get a dictionary output that can be later consumed very easily.\n",
    "\n",
    "First let's add the rest of entities that we want to extract from the document. Go ahead and un-comment (delete the `#` sign from the lines) the comented lines in the `output_template` below and execute the code cell. We will also try to split the paitent's first and last names into separate fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf45b99-079a-4b0f-9966-8edc28cdfe3f",
   "metadata": {},
   "source": [
    "In the following code cell, we use our `output_template` to create the format instruction text, and also initialize an `output_parser` that will be later used to parse the output.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>INSTRUCTION:</b> delete the \"#\" sign from the lines to un-comment, in the code block below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40abae8-bf93-4572-8e9c-cf8ac172ee97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "    \n",
    "class ExtractionEntities(BaseModel):\n",
    "    doctor_name: str = Field(description=\"The doctor or provider's full name\")\n",
    "    provider_id: str = Field(description=\"The doctor or provider's ID\")\n",
    "    # NOTICE: Here we have split the patient's first and last name\n",
    "    patient_first_name: str = Field(description=\"The patient's first and middle name\") \n",
    "    patient_last_name: str = Field(description=\"The patient's last name\")\n",
    "    patient_id: str = Field(description=\"The patient's ID\")\n",
    "    patient_gender: str = Field(description=\"The patient's gender\")\n",
    "    patient_age: str = Field(description=\"The patient's age\")\n",
    "    #  Un-comment the lines below\n",
    "    # admitted_date: str = Field(description=\"Date the patient was admitted to the hospital\")\n",
    "    # discharge_date: str = Field(description=\"Date the patient was discharged from the hospital\")\n",
    "    # discharged_to: str = Field(description=\"The disposition of where the patient was released or discharged to\")\n",
    "    # drug_allergies: str = Field(description=\"The patient's known drug allergies\")\n",
    "    \n",
    "    \n",
    "# output_parser = StructuredOutputParser.from_response_schemas(response_schems)\n",
    "output_parser = PydanticOutputParser(pydantic_object=ExtractionEntities)\n",
    "format_instructions= output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04966ddc-4549-4144-81c8-c807217ebb08",
   "metadata": {},
   "source": [
    "What we did above is basically used LangChain's `format instruction` method `get_format_instructions()` to build an instruction prompt from the desired template. This alleviates the need for us to build our own prompt using the template JSON every time a new entity is added. You can potentially store the template in a file and maintain it separately and the code would simply use the file to generate the prompt instruction using the template. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77752cff-321d-4bed-b622-76a63d80ac31",
   "metadata": {},
   "source": [
    "Our code here is pretty similar to before with the exception of the format instructions in the prompt template. We also instruct the model to strictly adhere to the format instructions when generating the output, so that our `output_parser` can parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d0b6e-8c71-48fb-b2e5-5232edd2a436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "You are a helpful assistant. Please extract the following details from the document and strictly follow the instructions described in the format instructions to format the output. Skip any preamble text and generate the final answer. Do not generate incomplete answer.\n",
    "\n",
    "<format_instructions>\n",
    "{format_instructions}\n",
    "</format_instructions>\n",
    "\n",
    "<document>\n",
    "{doc_text}\n",
    "<document>\n",
    "\n",
    "<final_answer>\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, \n",
    "                        input_variables=[\"doc_text\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bedrock_llm)\n",
    "output = llm_chain.run({\"doc_text\": full_text, \"details\": details, \"format_instructions\": format_instructions})\n",
    "\n",
    "parsed_output= output_parser.parse(output)\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e293d-3664-4e62-9dd1-fc5f4795eb14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output = dict(parsed_output)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b1210-7137-4ec0-b3d1-49702acd1f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output['doctor_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee97c4-7e0c-4f22-82c4-5650dda68c73",
   "metadata": {},
   "source": [
    "## Correction of incomplete generation by the LLM\n",
    "---\n",
    "\n",
    "In some cases the model may not generate the entire structured output as prompted resulting in incomplete output, resulting in an incomplete JSON or missing values. To make this more robust we can handle this using exception handling and re-prompting the model to rectify the output again. This makes for a more robust implementation for a production deployment. In the code cell below, if the `output_parser.parse()` method fails to parse the LLMs output, that would mean that the LLM generated an incomplete output. Subsequently, the `except` block of the code will catch the error and re-prompt the model to generate the full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e1bb4-a0b9-490c-a44c-b0ba54842542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain.schema import OutputParserException\n",
    "\n",
    "try:\n",
    "    parsed_output= output_parser.parse(output)\n",
    "except OutputParserException as e:\n",
    "    new_parser = OutputFixingParser.from_llm(\n",
    "        parser=output_parser,\n",
    "        llm=bedrock_llm\n",
    "    )\n",
    "    parsed_output = new_parser.parse(output)\n",
    "    \n",
    "final_output = dict(parsed_output)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a411c6-cdfa-40d1-98e7-ea7f964da770",
   "metadata": {},
   "source": [
    "# Value standardization\n",
    "---\n",
    "\n",
    "We were able to get structured key-values out of the document using the LLM so far. We would also like to standardize some of the outputs. For example we would like the dates in the output to be of DD/MM/YYYY format instead of DD-Mon-YYYY format. Let's see if we can quickly update the format instructions to achieve this.\n",
    "\n",
    "For the two date key's we have will add some additional instruction ` This should be formatted in DD/MM/YYYY format.`. Go ahead and modify the Field description such that it looks like below for both the date fields -\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>INSTRUCTION:</b> modify the Field description.Do this step for both the <b>addmitted_date</b> and <b>discharge_date</b> fields. For example - <br><br>\n",
    "    <b>Field(description=\"The patient's date of birth, this should be formatted in DD/MM/YYYY format\")</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86db149-30ac-41b3-b2df-aa0bbc08af25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from datetime import date\n",
    "\n",
    "class DateEntities(BaseModel):\n",
    "    # Modify the field description for the next two fields\n",
    "    admitted_date: str = Field(description=\"Date the patient was admitted to the hospital\")\n",
    "    discharge_date: str = Field(description=\"Date the patient was discharged from the hospital\")\n",
    "    ######\n",
    "    \n",
    "output_parser = PydanticOutputParser(pydantic_object=DateEntities)\n",
    "format_instructions= output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7961f-1cc8-4d61-96dc-3c886b819b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain.schema import OutputParserException\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "You are a helpful assistant. Please extract the following details from the document and strictly follow the instructions described in the format instructions and additional instructions to format the output. Skip any preamble text and generate the final answer. Do not generate incomplete answer.\n",
    "\n",
    "<format_instructions>\n",
    "{format_instructions}\n",
    "</format_instructions>\n",
    "\n",
    "<document>\n",
    "{doc_text}\n",
    "<document>\n",
    "\n",
    "<final_answer>\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, \n",
    "                        input_variables=[\"doc_text\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bedrock_llm)\n",
    "output = llm_chain.run({\"doc_text\": full_text, \"details\": details, \"format_instructions\": format_instructions})\n",
    "\n",
    "try:\n",
    "    parsed_output= output_parser.parse(output)\n",
    "except OutputParserException as e:\n",
    "    new_parser = OutputFixingParser.from_llm(\n",
    "        parser=output_parser,\n",
    "        llm=bedrock_llm\n",
    "    )\n",
    "    parsed_output = new_parser.parse(output)\n",
    "    \n",
    "final_output = dict(parsed_output)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c8185-0c44-4e3d-97fe-30ad5bc015c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(final_output['admitted_date'])\n",
    "print(final_output['discharge_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "\n",
    "We will perform cleanup at the end of the workshop\n",
    "\n",
    "## Conclusion\n",
    "---\n",
    "\n",
    "In this module, we performed structured data extraction from our document using an LLM with Amazon Bedrock and the text generated by Amazon Textract's LAYOUT feature. We first used a JSON to define a template, and then used prompt engineering techniques to get the desired output from the LLM. Then, we used a more cleaner and programmatic way to define the data template that we desire and used that to extract the data. We also looked at what to do in case the LLM generates incomplete results and re-prompting the model to complete the output using exception handling. Finally, we performed standardization of date formats by simply adding a formatting instruction to the description of the date fields. In the next module, we will perform Table self-querying and look at some of the basics of loading a document text into a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
