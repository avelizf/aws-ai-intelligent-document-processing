{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6e38fc-9f24-4c8f-9e2e-2c120eb68d45",
   "metadata": {},
   "source": [
    "# Module 2 - Document classification & summarization\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the `Data Science 3.0` image.\n",
    "</div>\n",
    "\n",
    "In this notebook, we demonstrate how you can integrate Amazon Textract with LangChain as a document loader to extract data from documents and use generative AI capabilities within the various IDP phases. We will perform the following with different LLMs.\n",
    "\n",
    "- Classification\n",
    "- Summarization\n",
    "- Spell check corrections\n",
    "\n",
    "For the documents, we will use samples that our workflow has processed in the previous notebook. The samples are preset in the directory `sample-docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd886e-d953-4b0e-81a1-54d72847b8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(f\"SageMaker bucket is {data_bucket}, and SageMaker Execution Role is {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021cbac-77d2-4acc-84bf-7c99a9c2a4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br = boto3.client('bedrock')\n",
    "resp = br.list_foundation_models(byProvider='anthropic')\n",
    "for model in resp['modelSummaries']:\n",
    "    print(model['modelId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c04bf-ae11-450c-8d1d-f6107e944f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"anthropic.claude-instant-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b0ee4-b856-4718-992e-99390f56068e",
   "metadata": {},
   "source": [
    "# 1. Document Classification\n",
    "---\n",
    "\n",
    "Classify a document based on it's content, given a list of classes. For this exercise, we will use the `sample-docs/mixed_sample_0.pdf` document. Let's take a look at the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a91590-8ea0-4712-87be-2c9359ee32b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "document_path=f\"s3://{data_bucket}/textract-linearized-output/uploads/mixed_sample_0\"\n",
    "IFrame(\"./sample-docs/mixed_sample_0.pdf\", width=600, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e4078-75b7-4432-9dec-7cae833e4107",
   "metadata": {},
   "source": [
    "As we can see, the document is a multi-page pdf and contains a variety of documents. More specifically it contains the following documents.\n",
    "\n",
    "- A bank statement\n",
    "- Patient discharge summary\n",
    "- Health Plan document\n",
    "- Doctor's notes\n",
    "- Driver's License\n",
    "- Invoice\n",
    "\n",
    "We will first load all the text that has been extracted and stored in S3 and then try to classify each page using an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7499d-12c8-4e3b-a5f4-567148009941",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from read_doc_from_s3 import read_document\n",
    "document = read_document(doc_path=document_path)\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd1a98-2fbe-4672-9855-db4759f29e7c",
   "metadata": {},
   "source": [
    "We have extracted the document text from S3, now let's define a list of classes for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b5f01-0266-406e-b54d-90e7ab8c1d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_classes = ['BANK_STMT','DISCHARGE_SUMMARY', 'HEALTH_PLAN', 'DOCTORS_NOTE', 'ID_DOCUMENT', 'INVOICE']\n",
    "classes = \",\".join(document_classes)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641327d-e375-4c03-ac1e-96704bcdbd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Given a list of classes, classify the document into one of these classes. Skip any preamble text and just give the class name.\n",
    "\n",
    "<classes>{classes}</classes>\n",
    "<document>{doc_text}<document>\n",
    "<classification>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"classes\",\"doc_text\"])\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bedrock_llm)\n",
    "\n",
    "for index, doc in enumerate(document):\n",
    "    class_name = llm_chain.run({\"classes\": classes, \"doc_text\": doc})\n",
    "    print(f\"Page {index+1} is a document of type {class_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed904147-9ed1-4db1-81ee-5a433624b204",
   "metadata": {},
   "source": [
    "# 2. Document Summarization\n",
    "---\n",
    "\n",
    "Summarize large pieces of text from a document into smaller, more coincise explanations. In this block we will perform a single page summary. We will select the `sample-docs/health_plan.pdf` document for summarization purpose. As before, lets look at the document and load it's extracted text from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdd288-09e7-41ca-87dd-cf1b1525fc94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "summary_document_path=f\"s3://{data_bucket}/textract-linearized-output/uploads/health_plan\"\n",
    "\n",
    "IFrame(\"./sample-docs/health_plan.pdf\", width=600, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0118cf0-7dce-4bc7-b35c-fc3fbc7ab860",
   "metadata": {},
   "source": [
    "Let's load this document's extracted text pages from S3. We will first select, just a single page (page-2) of this document and attempt to perform summarization. So let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfcec0-97be-4f79-a2c0-42531f501b34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from read_doc_from_s3 import read_document\n",
    "document = read_document(doc_path=summary_document_path)\n",
    "page_2 = document[1]\n",
    "print(page_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bb6d-9069-4f34-a684-4d7d1750e6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Given a full document, give me a concise summary. Skip any preamble text and just give the summary.\n",
    "\n",
    "<document>{doc_text}</document>\n",
    "<summary>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"doc_text\"])\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "num_tokens = bedrock_llm.get_num_tokens(page_2)\n",
    "print (f\"Our prompt has {num_tokens} tokens \\n\\n=========================\\n\")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bedrock_llm)\n",
    "summary = llm_chain.run(page_2)\n",
    "\n",
    "print(summary.replace(\"</summary>\",\"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4d20a-eef5-4147-ac82-647ee44ed467",
   "metadata": {},
   "source": [
    "# Multi-page summarization\n",
    "---\n",
    "\n",
    "We will now attempt to summarize the entire multi-page health plan document. Let's extract all pages of the document from our S3 location, but this time we will load each page in LangChain `Document` schema by passing `return_as=\"langchain_doc\"` to our `read_document` function. Go ahead and add that parameter to the function and execute the code cell and Notice the structure of `full_document` which is a list of `Document(...)` schema objects. We need the document in this format, as opposed to just plain text for the subsequent summary generation code that uses the `load_summarize_chain` method.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>INSTRUCTION:</b> Go ahead and add the new argument <b>return_as=\"langchain_doc\"</b> to the <b>read_document()</b> function and execute the code cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032d517-9601-4d9e-bf36-1608b8355488",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from read_doc_from_s3 import read_document\n",
    "\n",
    "# add the parameter to the read_document() function call below\n",
    "full_document = read_document(doc_path=summary_document_path) \n",
    "\n",
    "full_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befeda3c-1608-4b4f-9625-c02771cfe202",
   "metadata": {},
   "source": [
    "Since with 100k context we have a pretty healthy context window we don't need to further split this. Let's see the per page token size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adaa45-cd1d-4687-9911-50d7fa53b488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "num_docs = len(document)\n",
    "print (f\"There are {num_docs} pages in the document\")\n",
    "for index, doc in enumerate(full_document):\n",
    "    num_tokens_first_doc = bedrock_llm.get_num_tokens(doc.page_content)\n",
    "    print (f\"Page {index+1} has approx. {num_tokens_first_doc} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e988e759-4249-4166-b49b-921ec5f2ad12",
   "metadata": {},
   "source": [
    "We will use LangChain `load_summarize_chain` with a `map_reduce` chain type. For more information on Summarization techniques with LangChain refer to [this document](https://python.langchain.com/docs/use_cases/summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771fcf9-a7af-4161-bdb7-f605c4a4f5ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summary_chain = load_summarize_chain(llm=bedrock_llm, chain_type='map_reduce',\n",
    "                                     verbose=True # Set verbose=True if you want to see the prompts being used\n",
    "                                    )\n",
    "output = summary_chain.run(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca6a42-918a-4935-b237-59e2dc1fb477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b249e4-8028-4998-8d4a-7f72d7855b38",
   "metadata": {},
   "source": [
    "## 3. [BONUS] Spell check and corrections\n",
    "---\n",
    "\n",
    "This is a bonus and optional exercise. In this excercise, we perform grammatical and spelling corrections on text extracted from a hand written document. Let's load the document and it's extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a4f9a-2f8e-45b8-aa28-cbe4144e14f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "hand_written_doc_path=f\"s3://{data_bucket}/textract-linearized-output/uploads/hand_written_note\"\n",
    "\n",
    "IFrame(\"./sample-docs/hand_written_note.pdf\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99cbab8-11d6-4d90-bc05-d21cdb161f22",
   "metadata": {},
   "source": [
    "We only need the plain text of the document in this case, so let's load that from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d3d97-45fb-4373-b894-9f091a11a34e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from read_doc_from_s3 import read_document\n",
    "document = read_document(doc_path=hand_written_doc_path)\n",
    "print(document[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5dc3e-af8d-42ff-8de9-0542c7995759",
   "metadata": {},
   "source": [
    "As you can see the extracted text isn't very accurate and has some mistakes because of the poorly hand written document. Let's attempt to rectify this using an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3bd16-665a-4cd1-b520-46bfb39e0736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Given a detailed 'Document', perform spelling and grammatical corrections. Ensure the output is coherent, polished, and free from errors. Skip any preamble text and give the answer.\n",
    "\n",
    "<document>{doc_text}</<document>\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"doc_text\"])\n",
    "llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "try:\n",
    "    txt = document[0]\n",
    "    std_op = llm_chain.run({\"doc_text\": txt})\n",
    "    \n",
    "    print(\"Extracted text\")\n",
    "    print(\"==============\")\n",
    "    print(txt)\n",
    "\n",
    "    print(\"\\nCorrected text\")\n",
    "    print(\"==============\")\n",
    "    print(std_op.strip())\n",
    "    print(\"\\n\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497cb636-9ea2-4f3f-aa6e-a856574a2a17",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "\n",
    "We will perform cleanup at the end of the workshop\n",
    "\n",
    "## Conclusion\n",
    "---\n",
    "\n",
    "In this module, we performed document classification of a multi-page PDF document using the extracted text from the document. We also performed, document summarization of a single page document and a multi-page document to generate precise summaries. As a bonus we looked at a sample document that had poorly hand written text, and we extracted the text and got help from an LLM to rectify the grammatical and spelling mistakes. In the next module, we will explore on how to perform structured entity extractions from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fbd21-e126-4bde-9907-29d30ec0c9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
