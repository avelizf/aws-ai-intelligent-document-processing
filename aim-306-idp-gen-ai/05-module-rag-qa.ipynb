{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a084d0-2c06-4483-9601-dab9188f3efa",
   "metadata": {},
   "source": [
    "# Module 5 - In-context Q&A with with Retrieval Augmented Generation (RAG)\n",
    "____\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the \"Data Science 3.0\" image.\n",
    "</div>\n",
    "\n",
    "In this notebook we will walk through Q&A with a document first by extracting text from a document using Amazon Textract, generating chunks of text and store them into a Vector DB, and then performing Q&A with a Anthropic Claude model via Amazon Bedrock and get precise answers from the model. Later on, we will also implement a chat application with chat history to chat with documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27db6c0-202c-4cb1-b639-eb11cabbcbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "br = boto3.client('bedrock')\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(f\"SageMaker bucket is {data_bucket}, and SageMaker Execution Role is {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246f06a-8e8b-4db2-bffd-74d1ce7c8383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"anthropic.claude-instant-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919c918-efa5-446f-8504-de488ae22439",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Common sense reasoning and QA on a document\n",
    "\n",
    "In this section, we will perform common sense reasoning and Q&A on a document. This section does the following\n",
    "\n",
    "- Generates text from documents and stores them into S3 in plaintext format\n",
    "- Generate embeddings from the text\n",
    "- Uses an in-memory vector database to store the embeddings. In this case we will use [FAISS](https://ai.meta.com/tools/faiss/#:~:text=FAISS%20(Facebook%20AI%20Similarity%20Search,more%20scalable%20similarity%20search%20functions.).\n",
    "- Perform similarity search on the in-memory vector db to find relevant pieces of text that have relavancy to the asked question (by the user)\n",
    "- Generate the context for the LLM using the search results\n",
    "- Give the model the context and the original question asked\n",
    "- Get the answer back from the LLM\n",
    "- Profit\n",
    "\n",
    "> _\"Wait but that's a lot of steps just for getting an answer back? Why?\"_\n",
    "\n",
    "We would love to explain and dive deeper into why, but here's a paper that does a better job of explain the why? and the how? - https://arxiv.org/pdf/2005.11401.pdf . In short, LLMs know too much, _sometimes a bit too much that it may get confused and wander into the proverbial forest of it's own world knowledge and go start gathering firewood, when it was actually asked to go pick some fruit_. To solve this problem, and to get accurate/factual answers, we use this method of Retrieval-Augmented Generation (aka RAG), just to give the LLM a bit more _context_ to work with such that it gives us the desired output (like a fruit basket in our example, so that it knows it's only supposed to pick fruits) .\n",
    "\n",
    "As a first step, we read a file (document) using Amazon Textract using LangChain Textract Document Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d5f9e-3142-437b-a7ad-31bfd110a656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "qa_document_path=f\"s3://{data_bucket}/textract-linearized-output/uploads/health_plan\"\n",
    "\n",
    "IFrame(\"./sample-docs/health_plan.pdf\", width=600, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125a6d5-34f1-49be-b5f5-290cd495f1ae",
   "metadata": {},
   "source": [
    "Let's look at the extracted text from the S3 location where we extracted all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2f938-7fde-4cc3-898d-c75237f0f019",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from read_doc_from_s3 import read_document\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "document = read_document(doc_path=qa_document_path)\n",
    "\n",
    "full_text = \"\"\n",
    "for index,page in enumerate(document):\n",
    "    full_text += page.strip() + \"\\n\\n\" \n",
    "\n",
    "# display_markdown(full_text, raw=True)\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cbf51-1409-4d5e-9201-101290622fb1",
   "metadata": {},
   "source": [
    "Now that we have extracted the document, we split the document into smaller chunks, this is required because we may have a large multi-page document and our LLMs may have token limits. It will also ensure that we only get the relevant parts of the document to build the context instead of full page texts. Then these chunks will be loaded into the Vector DB for performing similarity search in the subsequent steps. \n",
    "\n",
    "However, before we store the document in the VectorDB, we will have to generate embeddings on the text. We will use Titan Embeddings G1 Text model for that purpose. Let's start by splitting the document into smaller chunks. We will use two levels of text splitting here -\n",
    "\n",
    "1. Since our extracted text is already in markdown formatted, we will use LangChain's [MarkdownHeaderTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/markdown_header_metadata), this splitter is built specially for markdown text and does chunking based on markdown headers (titles and subtitles marked by `#` and or `##`). This helps keep the text chunks related to the header of the paragraphs.\n",
    "2. Next, we take the markdown split text and further chunk it using LangChain's [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter). This splitter tries to split on a list of characters in order until the chunks are small enough. The default list of characters is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac59948-3b16-4f52-b2e1-38174fd0cad4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Title\"),\n",
    "    (\"##\", \"Subtitile\")\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(full_text)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,\n",
    "                                               chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(md_header_splits)\n",
    "for index, text in enumerate(texts):\n",
    "    print(f\"==== Chunk {index+1} ====\")\n",
    "    print(text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd295b2-5396-4058-81ab-c5516e8b6d3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We have split the document into smaller chunks. We will now perform a couple of things-\n",
    "\n",
    "- Generate embeddings of these chunks\n",
    "- Store these embeddings into a vector database\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec36bb-91ab-43f4-9331-2c4f812ca8d6",
   "metadata": {},
   "source": [
    "## Vector database\n",
    "\n",
    "This vector database is going to store the embeddings that we generate. This notebook uses ChromaDB and will be transient and in memory. ChromaDB is a vector store that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions. The VectorStore APIs that use ChromaDB within LangChain are available [here](https://python.langchain.com/docs/integrations/vectorstores/chroma). \n",
    "\n",
    "We will use Hugging Face Sentence Transformer embedding model to generate the embeddings and _Cosine similarity_ as a metric for similarity between two pieces of texts.\n",
    "\n",
    "> **_Cosine similarity_** is a metric used to measure how similar two vectors are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in $[0,1]$. \n",
    ">\n",
    ">The formula to calculate the cosine similarity `S` between two vectors `A` and `B` is:\n",
    ">\n",
    ">$[ S(A, B) = \\frac{A \\cdot B}{||A|| \\, ||B||} ]$\n",
    ">\n",
    ">Where:\n",
    ">- $A â‹… B$ is the dot product of the vectors $A$ and $B$\n",
    ">- $||A||$ and $||B||$ are the norms (or magnitudes) of vectors $A$ and $B$, respectively\n",
    ">\n",
    ">In text analysis, cosine similarity is often used to measure the similarity between two documents. It is calculated using the _term frequency-inverse document frequency_ (TF-IDF) weights of the terms in the documents.\n",
    ">\n",
    ">When the cosine similarity is close to 1, it indicates that the two vectors are very similar to each other. When it is close to 0, it indicates that the vectors are dissimilar. If the cosine similarity is 1, the vectors are identical (considering the direction, not the magnitude), and if it's -1, they are completely dissimilar. \n",
    ">\n",
    ">This metric is widely used in information retrieval and text mining to assess the similarity between documents or the relevance of documents to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48a990-1583-49c2-9b76-9a97c7175fcb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceEmbeddings(encode_kwargs=encode_kwargs)\n",
    "\n",
    "# we initalize ChromaDB collection with Cosine similarity\n",
    "vector_db = Chroma.from_documents(documents=texts, \n",
    "                                  embedding=embeddings, \n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a05a55-584a-4333-907f-b32fdeeba694",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> Since we are loading the Chroma Vector DB in memory, it will load into the SageMaker Studio instance's memory you may want to free up memory from time to time. To do that, uncomment the line below and execute this cell\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    <b>CAUTION:</b> The code cell below will delete the vector db index/collection.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ea83e-3eb0-44c3-93fa-13fcca9d82ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vector_db.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609e4cc-f5f2-4f44-b55c-da6ba0602698",
   "metadata": {},
   "source": [
    "We have loaded our vector db with the document, now let's run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408abd43-a7bb-438d-aaa5-ffbecb53ae5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the annual deductible per person?\"\n",
    "docs = vector_db.similarity_search(query)\n",
    "\n",
    "print(f\"=========Following are the text chunks relevant to the question - '{query}'=========\\n\")\n",
    "for doc in docs:\n",
    "    print(f\"Text chunk: {doc.page_content}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff123f07-ff05-4ee8-a688-c819798b17ec",
   "metadata": {},
   "source": [
    "The query returns all the chunks from the document that is similar to the query, by default it returns the Top 4 similar chunks. Let's see how to return just Top 3 with confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c20e7-d676-42f0-9f5b-4a3c88658b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vector_db.similarity_search_with_score(query, k = 3)\n",
    "\n",
    "print(f\"=========Following are the text chunks (with similarity scores) relevant to the question - '{query}'=========\\n\")\n",
    "for doc in docs:\n",
    "    print(f\"Text chunk: {doc[0].page_content}\")    \n",
    "    print(f\"Similarity Score: {1 - doc[1]}\") # since cosine_distane = (1 - cosine_similarity)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6f088-6591-4671-8602-cfd7446a7059",
   "metadata": {},
   "source": [
    "## Vector store-backed retriever\n",
    "---\n",
    "\n",
    "According to LangChain documentation-\n",
    "\n",
    ">A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.\n",
    "\n",
    "Wrapping our vector db in a retriever wrapper is going to be useful when we use it in the Q&A chain for our chatbot in subsequent sections. But let's take a look how it works. The functionality is pretty similar to before (i.e. querying) with a slightly different interface.\n",
    "\n",
    "We first define a retriever with search type `similarity_score_threshold`, other option is mmr (max marginal relevancy). Note that the search_type depends on which vector DB you are using, some vector DBs may or may not support mmr etc.\n",
    "\n",
    ">similarity_score_threshold considers the cosine similarity of the reference text vs the embeddings.\n",
    "\n",
    "We also define how many top results to return, in this case 5. Finally we query the retriever using `get_relevant_documents` by passing in the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75255fb-dd71-4c8c-94df-6865ca9654a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "query = \"What is the total pharmacy out-of-pocket?\"\n",
    "\n",
    "# At the top we initialized the Chroma DB with cosine relevance function\n",
    "# here score_threshold is the cosine distance and not the similarity\n",
    "# cosine_distane = 1 - cosine_similarity\n",
    "# 0.5 is 1 - 0.5\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.5, \"k\": 5})\n",
    "relevant_docs = retriever.get_relevant_documents(query)   \n",
    "relevant_docs\n",
    "\n",
    "for doc in relevant_docs:\n",
    "    if 'Title' in doc.metadata:\n",
    "        print(f\"Title: {doc.metadata['Title']}\")\n",
    "    if 'Subtitile' in doc.metadata:\n",
    "        print(f\"Subtitile: {doc.metadata['Subtitile']}\")\n",
    "    print(f\"Text chunk: {doc.page_content}\")    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcf2f3-635f-4a04-9af5-e47f7d205bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "query = \"What is the color of the sky?\"\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.5, \"k\": 5})\n",
    "relevant_docs_u = retriever.get_relevant_documents(query)   \n",
    "relevant_docs_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b06ac4-d37d-4f02-9d2f-2f52b9ed5432",
   "metadata": {},
   "source": [
    "## Build context from retrieved documents\n",
    "---\n",
    "\n",
    "We now have the two relevant pieces of text that \"contain\" the anwer to our question, we are not quite there yet. So we will use a technique that we used earlier to build context and ask the quetion to the Llama-2 model. In this case, we will use the two text chunks we retrieved from the vector db to create the context by simply concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e44f51-d5de-46eb-9121-9d55305e1a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_context = str()\n",
    "for doc in relevant_docs:\n",
    "    full_context += doc.page_content+\" \"\n",
    "    \n",
    "print(full_context.strip(\".\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc69ef5-6995-4dff-b1ec-27e4ad44321a",
   "metadata": {},
   "source": [
    "The similarity seach query gave us a good output but we want some more key details out of it. Let's use an LLM to ask this question, but this time using the context that we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465a219-af2c-4982-a22d-e864e6c262c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Skip any preamble text and reasoning and give just the answer.\n",
    "\n",
    "<text>{document}</text>\n",
    "<question>{question}</question>\n",
    "<answer>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bedrock_llm)\n",
    "answer = llm_chain.run(document=full_context, question=\"What is the per-person pharmacy out-of-pocket?\")\n",
    "print(answer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dced583-30e0-4b17-8423-b78fbfd5526a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's run it with a different question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b80866-508b-4cda-8f9e-c295d251358b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = llm_chain.run(document=full_context, question=\"Who is the administrator for this plan?\")\n",
    "print(answer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc17dc-69a5-4fc5-8414-e01149c70f9e",
   "metadata": {},
   "source": [
    "The model doesn't know the answer because our context in `full_context` has no information about the administrator of the plan, and we asked the model to strictly answer from within the provided context. This means we will have to run a similarity search on the Vector database again using our new question, create the full context again, and then ask the question. Thankfully, LangChain makes it easy for us and we will see how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c706a-e00f-4d8e-99ee-b7e87fa385ff",
   "metadata": {},
   "source": [
    "### Performing Q&A with RAG with `load_qa_chain`\n",
    "---\n",
    "\n",
    "For this purpose, we will first define a question, and then generate embeddings from it. Once we have that we can perform similarity search on the vector database to find relevant pieces of information from the document. These relevant pieces of information will then be passed on to the model so that it can answer the question. We will use LangChain's `load_qa_chain` to perform Q&A with the model. The load qa chain does the work with prompt creation and all the context generation with help from the vector database.\n",
    "\n",
    "NOTE: In order to use the `RetrievalQA` from LangChain, your prompt template must have the two variables `context` and `question`. Using any other variable names will cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec529-b5c3-40c5-9ae1-a4a00e147827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.5, \"k\": 3})\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Skip any preamble text and reasoning and give just the answer.\n",
    "\n",
    "<text>{context}</text>\n",
    "<question>{question}</question>\n",
    "<answer>\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "# initialize the LLM\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=MODEL_ID, model_kwargs={'temperature':0})\n",
    "\n",
    "chain_type_kwargs = { \"prompt\": qa_prompt, \"verbose\": False } # change verbose to True if you need to see what's happening\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=bedrock_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=False,  # Change this to True if you want to see the sources being used by the LLM to answer the question\n",
    "    verbose=False # change verbose to True if you need to see what's happening\n",
    ")\n",
    "\n",
    "question=\"Who is the administrator for this plan?\"\n",
    "\n",
    "result = qa(question)\n",
    "print(\"\\n============ Answer ============\\n\")\n",
    "print(result[\"result\"].strip())\n",
    "\n",
    "if \"source_documents\" in result:\n",
    "    print(\"\\n============ Sources ============\\n\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"{doc.page_content}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624d807-ef0c-4be9-923f-af90ea03e416",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-warning\"> \n",
    "    <b>NOTE:</b> Change <code>return_source_documents</code> to <code>True</code> in the code cell above in the <code>RetrievalQA.from_chain_type()</code> call if you want to see the sources (text chunks) being used by the LLM to answer the question\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f1f81-bf70-434a-beae-2375813d76ab",
   "metadata": {},
   "source": [
    "Perfect! our model now can precisely answer the question. But how did it work?\n",
    "\n",
    "- First, the question text was taken and the embedding was generated using the Amazon Titan embedding model. This all happened inside the `retriever` as we defined earler with `retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 5})` our `vector_db` is a FAISS object that was initialized with Amazon Titan embedding model.\n",
    "- Next the `RetrievalQA` chain runs a similarity search with the generated embdeddings (from the question) to find out relevant pieces of text that are similar to the question we are try to get an answer for.\n",
    "- Then the chain builds the full context using the returned chunks and generates the full prompt using the `qa_prompt` template we provided.\n",
    "- Finally, the chain invokes the model to get the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f63ef-fcc6-48fa-976a-53a0be189ad5",
   "metadata": {},
   "source": [
    "## Chat with your document\n",
    "---\n",
    "\n",
    "We will now create a simple chat application to chat with our document. This application will not only perform in-context Q&A, but will also be able to answer questions based on chat history. For the chatbot we need `context management, history, vector stores, and many other things`. We will start by with a ConversationalRetrievalChain\n",
    "\n",
    "This uses conversation memory and RetrievalQAChain which Allow for passing in chat history which can be used for follow up questions.Source: https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "\n",
    "_We will use Gradio to quickly spin up our chat interface. So we will install Gradio next. Then we will define our Conversation chain and plug that into the chat application._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65ea53-71ee-4f11-a4f4-55f459a58f88",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --disable-pip-version-check --root-user-action=ignore gradio --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40817bbd-9e39-4100-97d1-4086541f50c0",
   "metadata": {},
   "source": [
    "We will also initialize our Claude model with `temperature=0` for less creative responses, and some stop words so that the model knows when to stop generating tokens. We initialize two instances of the LLM, one is used to rephrase the question by looking at the chat history (`bedrock_llm_condense`), and the other is used to answer questions using the RAG generated context `bedrock_llm`. You can potentially use the same LLM model instance for both, but this demonstrates that you can use two different models for your conversational interface to perform different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3955d-1636-4d90-af1b-d5d390d196a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bedrock_llm_condense = Bedrock(client=bedrock, \n",
    "                      model_id=MODEL_ID, \n",
    "                      model_kwargs={\"temperature\": 0,\"stop_sequences\": [\"\\n\\nHuman:\",\"</standalone_question>\"]})\n",
    "\n",
    "bedrock_llm = Bedrock(client=bedrock, \n",
    "                      model_id=MODEL_ID, \n",
    "                      model_kwargs={\"temperature\": 0.3,\"stop_sequences\": [\"\\n\\nHuman:\",\"</answer>\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad5a39-c2f5-4448-880b-69269d6f0c3a",
   "metadata": {},
   "source": [
    "To build our chat application, we will use a built-in LangChain chain called `ConversationalRetrievalChain`. This chain allows us to build conversational interface that is capable of retaining chat history, and perform RAG on our vector DB retriever simultaneously, without us having to code each of those steps individually. The purpose of the chat history, is when provided as a context to the model, the model will recall the conversation and may enrich the responses (with the help of additional context retrieved by the `retriever`) based on the current question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5b1ae-e472-4a29-a796-60becd9b79e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_chat_history(inputs) -> str:\n",
    "    res = []\n",
    "    for human, ai in inputs:\n",
    "        res.append(f\"Human:{human}\\nAssistant:{ai}\")\n",
    "    return \"\\n\".join(res)\n",
    "\n",
    "def create_prompt_template():\n",
    "    _template = \"\"\"\n",
    "    \n",
    "Given the following chat history and a follow up question, in its original language, rephrase the follow up question to be a standalone question but don't change its meaning. \n",
    "Skip the preamble and just get to the question.\n",
    "\n",
    "<chat_history>    \n",
    "{chat_history}\n",
    "</chat_history>    \n",
    "\n",
    "<follow_up_question>\n",
    "{question}\n",
    "</follow_up_question>\n",
    "\n",
    "<standalone_question>\n",
    "\"\"\"\n",
    "    conversation_prompt = PromptTemplate.from_template(_template)\n",
    "    return conversation_prompt\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "<human>\n",
    "Answer the question as truthfully as possible strictly using only the provided document and the human-ai chat history, in its original language, \n",
    "and if the answer is not contained within the document, say \"I don't know\". Skip any preamble text and reasoning and give just the \n",
    "answer. If the user greets you, just greet them back. Always respond in English.\n",
    "</human>\n",
    "\n",
    "<document>\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "</document>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\",\"chat_history\"])\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.4, \"k\": 5})\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=bedrock_llm, \n",
    "                                           retriever=retriever, \n",
    "                                           condense_question_prompt=create_prompt_template(),\n",
    "                                           condense_question_llm = bedrock_llm_condense,\n",
    "                                           combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    "                                           get_chat_history=get_chat_history,\n",
    "                                           verbose=False    # uncomment this to see logs\n",
    "                                          )\n",
    "\n",
    "questions = [\n",
    "    \"Hi, my name is John Doe, I have a family of 4 members.\",\n",
    "    \"Who is the plan administrator for this plan?\",\n",
    "    \"What is the annual deductible per person?\",\n",
    "    \"What last name should I use in the form and how much total for all of my family member would it cost?\"\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\":chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer'].strip()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd5768-1705-4d16-ad86-e1ea342db36f",
   "metadata": {},
   "source": [
    "We just had an automated chat session with a bunch of pre-determined questions and we also noticed that from the fourth question, the model is able to answer the name since we have access to the chat history. Keep in mind, as the chat session goes longer, the chat history can get bigger and bigger . In such cases, it is important to limit how far you want to remember the chat so that you don't run out of token limits, and encounter slower responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a804e26-2859-4381-9042-1b4eeb054a8a",
   "metadata": {},
   "source": [
    "## The Chat App with Gradio\n",
    "---\n",
    "\n",
    "Next we will build a simple chat app using Gradio and the same method we used above using `ConversationalRetrievalChain` and our vector database as a retriever. Note that our vector database is currently loaded with only one document. But you can imagine that you could have any number of documents loaded into the vector database.\n",
    "\n",
    "Once you run the following code cell, here are some questions you can ask in the chat interface-\n",
    "\n",
    "- Who is the plan Administrator?\n",
    "- Who are the third party administrator?\n",
    "- What is the per-person deductible?\n",
    "- What is ERISA?\n",
    "- Do you remember my name?       ---> Test if the bot remembers your name\n",
    "- Based on your previous answers, who are the primary and the third party administrators of the plan? ---> Test chat history\n",
    "- What is Co-pay?\n",
    "- What is a deductible?\n",
    "- What is the co-pay maximum for a family?  ---> Info not in the document\n",
    "- What is the co-pay maximum for a person?  ---> Info not in the document\n",
    "- What is the deductible maximum for a family?\n",
    "- what is the maximum out of pocket for pharmacy for a person?\n",
    "- what is the maximum out of pocket for pharmacy for a family?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b1979-df5c-4f50-9d07-8e3c38d3bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_chat_history(inputs) -> str:\n",
    "    res = []\n",
    "    for human, ai in inputs:\n",
    "        res.append(f\"Human:{human}\\nAssistant:{ai}\")\n",
    "    return \"\\n\".join(res)\n",
    "\n",
    "def create_prompt_template():\n",
    "    _template = \"\"\"\n",
    "    \n",
    "Given the following chat history and a follow up question, in its original language, rephrase the follow up question to be a standalone question but don't change its meaning. \n",
    "Skip the preamble and just get to the question.\n",
    "\n",
    "<chat_history>    \n",
    "{chat_history}\n",
    "</chat_history>    \n",
    "\n",
    "<follow_up_question>\n",
    "{question}\n",
    "</follow_up_question>\n",
    "\n",
    "<standalone_question>\n",
    "\"\"\"\n",
    "    conversation_prompt = PromptTemplate.from_template(_template)\n",
    "    return conversation_prompt\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "<human>\n",
    "Answer the question as truthfully as possible strictly using only the provided document and the human-ai chat history, in its original language, \n",
    "and if the answer is not contained within the document, say \"I don't know\". Skip any preamble text and reasoning and give just the \n",
    "answer. If the user greets you, just greet them back. Always respond in English.\n",
    "</human>\n",
    "\n",
    "<document>\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "</document>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\",\"chat_history\"])\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.4, \"k\": 5})\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=bedrock_llm, \n",
    "                                           retriever=retriever, \n",
    "                                           condense_question_prompt=create_prompt_template(),\n",
    "                                           condense_question_llm = bedrock_llm_condense,\n",
    "                                           combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    "                                           get_chat_history=get_chat_history,\n",
    "                                           verbose=False    # uncomment this to see logs\n",
    "                                          )\n",
    "chat_history = []\n",
    "\n",
    "def qa_fn(message, history):\n",
    "    result = qa({\"question\": message, \"chat_history\":chat_history})\n",
    "    chat_history.append((message, result[\"answer\"]))\n",
    "    return result['answer'].strip()\n",
    "\n",
    "gr.ChatInterface(qa_fn).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239af7b-2181-4492-9572-4d2541f6a54b",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "\n",
    "Let's clean up the documents we uploaded to S3 earlier in Module 1 and also delete the vector db collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f500b-778f-4ed6-a6e2-d845131ee5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb84d02-0b63-4cb4-bfbe-ca54046db668",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm s3://{data_bucket} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shut down the Kernel in this notebook. Follow the steps below to shut down the Notebook kernel.\n",
    "\n",
    "- Click on the \"_Kernel_\" menu on the top\n",
    "- Click on the \"_Shut Down Kernel_\" option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ec4b2-5c23-4fa3-9c11-6016fa8418f8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "---\n",
    "\n",
    "Thanks for joining us in this workshop! In this workshop -\n",
    "\n",
    "1. You learnt about the various API calls of Amazon Textract\n",
    "2. You exctracted layout linearized text from documents using Amazon Textract as well as plain text, and compared the two\n",
    "3. You bulk processed a large number of files by uploading them into an S3 bucket\n",
    "4. You reviewed the AWS Step Functions workflow to see how the documents were being processed.\n",
    "5. You did a number of exercises with Amazon Bedrock and generative AI (LLM), with Anthropic Claude Instant v1 model such as\n",
    "   - Single and multi-page document classification\n",
    "   - Single page and multi-page summarization\n",
    "   - Structured data extraction using templates, and Structured output parser with LangChain\n",
    "   - Performed Q&A on table data from documents, and performed table self-querying with the LLM and LangChain\n",
    "   - Finally, you performed document Q&A with RAG, and we built a chat-bot to chat with our documents\n",
    "\n",
    "For more resources on intelligent document processing and generative AI visit the links below-\n",
    "\n",
    "1. [Document processing at scale with IDP CDK constructs, samples](https://github.com/aws-solutions-library-samples/guidance-for-low-code-intelligent-document-processing-on-aws)\n",
    "2. [Guidance for Low Code Intelligent Document Processing on AWS](https://aws.amazon.com/solutions/guidance/low-code-intelligent-document-processing-on-aws/)\n",
    "3. [Intelligent Document Processing with AWS AI Services workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/c2af04b2-54ab-4b3d-be73-c7dd39074b20/en-US)\n",
    "4. [New Tools for Building with Generative AI on AWS.](https://aws.amazon.com/blogs/machine-learning/intelligent-document-processing-with-amazon-textract-amazon-bedrock-and-langchain/?utm_content=bufferfda52&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer#:~:text=New%20Tools%20for%20Building%20with%20Generative%20AI%20on%20AWS.)\n",
    "5. Read the blog - [Intelligent document processing with Amazon Textract, Amazon Bedrock, and LangChain](https://aws.amazon.com/blogs/machine-learning/intelligent-document-processing-with-amazon-textract-amazon-bedrock-and-langchain/?utm_content=bufferfda52&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)\n",
    "\n",
    "## Don't forget to complete the session survey in the mobile app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08d7a8-69da-4392-b873-34d4a19fe687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
