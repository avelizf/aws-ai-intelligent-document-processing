{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a084d0-2c06-4483-9601-dab9188f3efa",
   "metadata": {},
   "source": [
    "# Module 5 - In-context Q&A with with Retrieval Augmented Generation (RAG)\n",
    "____\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the \"Data Science 3.0\" image.\n",
    "</div>\n",
    "\n",
    "In this notebook we will walk through Q&A with a document first by extracting text from a document using Amazon Textract, generating chunks of text and store them into a Vector DB, and then performing Q&A with a Anthropic Claude model via Amazon Bedrock and get precise answers from the model. Later on, we will also implement a chat application with chat history to chat with documents.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You can ignore any WARNINGS during the `pip installs`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ba775-1b63-4c23-b71e-2ae1a6676b82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 langchain faiss-cpu transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27db6c0-202c-4cb1-b639-eb11cabbcbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "br = boto3.client('bedrock')\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(f\"SageMaker bucket is {data_bucket}, and SageMaker Execution Role is {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85e962-e28e-4a1e-91f8-e7b9e527d894",
   "metadata": {},
   "source": [
    "## Upload sample data to S3 bucket\n",
    "\n",
    "\n",
    "The sample document is in `/samples` directory. For this workshop, we will be using a sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f868164-f14e-4f54-99ae-fbdab7dfab5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload images to S3 bucket:\n",
    "\n",
    "!aws s3 cp samples s3://{data_bucket}/idp/genai --recursive --only-show-errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c640a5d-55d6-48ce-827e-d6176adc0075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://{data_bucket}/idp/genai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919c918-efa5-446f-8504-de488ae22439",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Common sense reasoning and QA on a document\n",
    "\n",
    "In this section, we will perform common sense reasoning and Q&A on a document. This section does the following\n",
    "\n",
    "- Generates text from documents and stores them into S3 in plaintext format\n",
    "- Generate embeddings from the text\n",
    "- Uses an in-memory vector database to store the embeddings. In this case we will use [FAISS](https://ai.meta.com/tools/faiss/#:~:text=FAISS%20(Facebook%20AI%20Similarity%20Search,more%20scalable%20similarity%20search%20functions.).\n",
    "- Perform similarity search on the in-memory vector db to find relevant pieces of text that have relavancy to the asked question (by the user)\n",
    "- Generate the context for the LLM using the search results\n",
    "- Give the model the context and the original question asked\n",
    "- Get the answer back from the LLM\n",
    "- Profit\n",
    "\n",
    "> _\"Wait but that's a lot of steps just for getting an answer back? Why?\"_\n",
    "\n",
    "We would love to explain and dive deeper into why, but here's a paper that does a better job of explain the why? and the how? - https://arxiv.org/pdf/2005.11401.pdf . In short, LLMs know too much, _sometimes a bit too much that it may get confused and wander into the proverbial forest of it's own world knowledge and go start gathering firewood, when it was actually asked to go pick some fruit_. To solve this problem, and to get accurate/factual answers, we use this method of Retrieval-Augmented Generation (aka RAG), just to give the LLM a bit more _context_ to work with such that it gives us the desired output (like a fruit basket in our example, so that it knows it's only supposed to pick fruits) .\n",
    "\n",
    "As a first step, we read a file (document) using Amazon Textract using LangChain Textract Document Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d5f9e-3142-437b-a7ad-31bfd110a656",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "loader = AmazonTextractPDFLoader(f\"s3://{data_bucket}/idp/genai/health_plan.pdf\")\n",
    "document = loader.load()\n",
    "print(f\"Textract extracted {len(document)} pages from the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125a6d5-34f1-49be-b5f5-290cd495f1ae",
   "metadata": {},
   "source": [
    "Let's look at the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2f938-7fde-4cc3-898d-c75237f0f019",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index,page in enumerate(document):\n",
    "    print(f\"=========Page {index+1}==========\")\n",
    "    print(page.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cbf51-1409-4d5e-9201-101290622fb1",
   "metadata": {},
   "source": [
    "Now that we have extracted the document, we split the document into smaller chunks, this is required because we may have a large multi-page document and our LLMs may have token limits. It will also ensure that we only get the relevant parts of the document to build the context instead of full page texts. Then these chunks will be loaded into the Vector DB for performing similarity search in the subsequent steps. \n",
    "\n",
    "However, before we store the document in the VectorDB, we will have to generate embeddings on the text. We use `HuggingFaceEmbeddings`  which is built into LangChain, for that purpose. For other models you may chose embedding models accordingly as suggested by the model provider. Let's start by splitting the document into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f860e-f26e-468d-963f-ca7c557435c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "                                               chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(document)\n",
    "\n",
    "for index, text in enumerate(texts):\n",
    "    print(f\"==== Chunk {index+1}, From Page {text.metadata['page']} ====\")\n",
    "    print(text.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd295b2-5396-4058-81ab-c5516e8b6d3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We have split the document into smaller chunks. We will now perform a couple of things-\n",
    "\n",
    "- Generate embeddings of these chunks\n",
    "- Store these embeddings into a vector database\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec36bb-91ab-43f4-9331-2c4f812ca8d6",
   "metadata": {},
   "source": [
    "## Vector database\n",
    "\n",
    "This vector database is going to store the embeddings that we generate. This notebook showcases FAISS and will be transient and in memory. FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions. The VectorStore APIs that use FAISS within LangChain are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html). \n",
    "\n",
    "We will use Amazon Titan embedding model to generate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820484c7-d1c8-4fb8-9f68-9a57dd57da57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = br.list_foundation_models(\n",
    "    byOutputModality='EMBEDDING'\n",
    ")\n",
    "for model in resp['modelSummaries']:\n",
    "    print(model['modelId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca8adf-14ee-4b3e-a07c-a3bff68b1c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings(client=bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c83ce7-bda6-4921-8a73-21d67b3a4a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Ensure that you have enabled amazon.titan-embed-text-v1 model in Amazon Bedrock console\n",
    "embeddings = BedrockEmbeddings(client=bedrock,model_id=\"amazon.titan-embed-text-v1\")\n",
    "vector_db = FAISS.from_documents(documents=texts, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ea83e-3eb0-44c3-93fa-13fcca9d82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are loading the FAISS Vector DB in memory, it will load into the SageMaker Studio instance's memory\n",
    "# you may want to free up memory from time to time. To do that, uncomment the line below and execute this cell\n",
    "\n",
    "# CAUTION! This will delete the vector index\n",
    "\n",
    "# vector_db.delete([vector_db.index_to_docstore_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609e4cc-f5f2-4f44-b55c-da6ba0602698",
   "metadata": {},
   "source": [
    "We have loaded our vector db with the document, now let's run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408abd43-a7bb-438d-aaa5-ffbecb53ae5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the annual deductible per person?\"\n",
    "docs = vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61612dd9-08d1-4cf6-b13c-9c607550f52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff123f07-ff05-4ee8-a688-c819798b17ec",
   "metadata": {},
   "source": [
    "The query returns all the chunks from the document that is similar to the query, by default it returns the Top 4 similar chunks. Let's see how to return just Top 3 with confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c20e7-d676-42f0-9f5b-4a3c88658b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vector_db.similarity_search_with_score(query, k = 3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6f088-6591-4671-8602-cfd7446a7059",
   "metadata": {},
   "source": [
    "## Vector store-backed retriever\n",
    "---\n",
    "\n",
    "According to LangChain documentation-\n",
    "\n",
    ">A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.\n",
    "\n",
    "Wrapping our vector db in a retriever wrapper is going to be useful when we use it in the Q&A chain for our chatbot in subsequent sections. But let's take a look how it works. The functionality is pretty similar to before (i.e. querying) with a slightly different interface.\n",
    "\n",
    "We first define a retriever with search type mmr (Max Marginal Relevance), other option is similarity. Note that the search_type depends on which vector DB you are using, some vector DBs may or may not support mmr etc.\n",
    "\n",
    ">MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document.\n",
    "\n",
    "We also define how many top results to return, in this case 2. Finally we query the retriever using get_relevant_documents by passing in the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75255fb-dd71-4c8c-94df-6865ca9654a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the total pharmacy out-of-pocket?\"\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 3})\n",
    "relevant_docs = retriever.get_relevant_documents(query)   \n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b06ac4-d37d-4f02-9d2f-2f52b9ed5432",
   "metadata": {},
   "source": [
    "## Build context from retrieved documents\n",
    "---\n",
    "\n",
    "We now have the two relevant pieces of text that \"contain\" the anwer to our question, we are not quite there yet. So we will use a technique that we used earlier to build context and ask the quetion to the Llama-2 model. In this case, we will use the two text chunks we retrieved from the vector db to create the context by simply concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e44f51-d5de-46eb-9121-9d55305e1a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_context = str()\n",
    "for doc in relevant_docs:\n",
    "    full_context += doc.page_content+\" \"\n",
    "    \n",
    "print(full_context.strip(\".\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc69ef5-6995-4dff-b1ec-27e4ad44321a",
   "metadata": {},
   "source": [
    "The similarity seach query gave us a good output but we want some more key details out of it. Let's use an LLM to ask this question, but this time using the context that we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465a219-af2c-4982-a22d-e864e6c262c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "loader = AmazonTextractPDFLoader(\"./samples/discharge-summary.png\")\n",
    "document = loader.load()\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Skip any preamble text and reasoning and give just the answer.\n",
    "\n",
    "<text>{document}</text>\n",
    "<question>{question}</question>\n",
    "<answer>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=\"anthropic.claude-v1\")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bedrock_llm)\n",
    "answer = llm_chain.run(document=full_context, question=\"What is the per-person pharmacy out-of-pocket?\")\n",
    "print(answer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dced583-30e0-4b17-8423-b78fbfd5526a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's run it with a different question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b80866-508b-4cda-8f9e-c295d251358b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = llm_chain.run(document=full_context, question=\"Who is the administrator for this plan?\")\n",
    "print(answer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc17dc-69a5-4fc5-8414-e01149c70f9e",
   "metadata": {},
   "source": [
    "The model doesn't know the answer because our context in `full_context` has no information about the administrator of the plan, and we asked the model to strictly answer from within the provided context. This means we will have to run a similarity search on the Vector database again using our new question, create the full context again, and then ask the question. Thankfully, LangChain makes it easy for us and we will see how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c706a-e00f-4d8e-99ee-b7e87fa385ff",
   "metadata": {},
   "source": [
    "### Performing Q&A with RAG with `load_qa_chain`\n",
    "---\n",
    "\n",
    "For this purpose, we will first define a question, and then generate embeddings from it. Once we have that we can perform similarity search on the vector database to find relevant pieces of information from the document. These relevant pieces of information will then be passed on to the model so that it can answer the question. We will use LangChain's `load_qa_chain` to perform Q&A with the model. The load qa chain does the work with prompt creation and all the context generation with help from the vector database.\n",
    "\n",
    "NOTE: In order to use the `RetrievalQA` from LangChain, your prompt template must have the two variables `context` and `question`. Using any other variable names will cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec529-b5c3-40c5-9ae1-a4a00e147827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 3})\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Skip any preamble text and reasoning and give just the answer.\n",
    "\n",
    "<text>{context}</text>\n",
    "<question>{question}</question>\n",
    "<answer>\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "chain_type_kwargs = { \"prompt\": qa_prompt, \"verbose\": False } # change verbose to True if you need to see what's happening\n",
    "\n",
    "bedrock_llm = Bedrock(client=bedrock, model_id=\"anthropic.claude-v1\")\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=bedrock_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=False # change verbose to True if you need to see what's happening\n",
    ")\n",
    "\n",
    "question=\"Who is the administrator for this plan?\"\n",
    "\n",
    "result = qa.run(question)\n",
    "print(result.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f1f81-bf70-434a-beae-2375813d76ab",
   "metadata": {},
   "source": [
    "Perfect! our model now can precisely answer the question. But how did it work?\n",
    "\n",
    "- First, the question text was taken and the embedding was generated using the Amazon Titan embedding model. This all happened inside the `retriever` as we defined earler with `retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 3})` our `vector_db` is a FAISS object that was initialized with Amazon Titan embedding model.\n",
    "- Next the `RetrievalQA` chain runs a similarity search with the generated embdeddings (from the question) to find out relevant pieces of text that are similar to the question we are try to get an answer for.\n",
    "- Then the chain builds the full context using the returned chunks and generates the full prompt using the `qa_prompt` template we provided.\n",
    "- Finally, the chain invokes the model to get the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f63ef-fcc6-48fa-976a-53a0be189ad5",
   "metadata": {},
   "source": [
    "## Chat with your document\n",
    "---\n",
    "\n",
    "We will now create a simple chat application to chat with our document. This application will not only perform in-context Q&A, but will also be able to answer questions based on chat history. For the chatbot we need `context management, history, vector stores, and many other things`. We will start by with a ConversationalRetrievalChain\n",
    "\n",
    "This uses conversation memory and RetrievalQAChain which Allow for passing in chat history which can be used for follow up questions.Source: https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "\n",
    "_We will use Gradio to quickly spin up our chat interface. So we will install Gradio next. Then we will define our Conversation chain and plug that into the chat application._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65ea53-71ee-4f11-a4f4-55f459a58f88",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea4492-4f52-4ed0-b671-48d933ac5856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's delete the index, we will create it again\n",
    "vector_db.delete([vector_db.index_to_docstore_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40817bbd-9e39-4100-97d1-4086541f50c0",
   "metadata": {},
   "source": [
    "We will now read a document using `AmazonTextractPDFLoader` split the pages into smaller chunks, generate embeddings using Amazon Titan Embedding model, and load it into our Vector DB. We will also initialize our Claude model with `temperature=0.1` for less diversified responses, and some stop words so that the model knows when to stop generating tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3955d-1636-4d90-af1b-d5d390d196a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "\n",
    "loader = AmazonTextractPDFLoader(f\"s3://{data_bucket}/idp/genai/health_plan.pdf\")\n",
    "document = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "                                               chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(document)\n",
    "\n",
    "# Ensure that you have enabled amazon.titan-embed-text-v1 model in Amazon Bedrock console\n",
    "embeddings = BedrockEmbeddings(client=bedrock,model_id=\"amazon.titan-embed-text-v1\")\n",
    "vector_db = FAISS.from_documents(documents=texts,embedding=embeddings)\n",
    "\n",
    "bedrock_llm = Bedrock(client=bedrock, \n",
    "                      model_id=\"anthropic.claude-v1\", \n",
    "                      model_kwargs={\"temperature\": 0.1,\"stop_sequences\": [\"\\n\\nHuman:\",\"</answer>\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad5a39-c2f5-4448-880b-69269d6f0c3a",
   "metadata": {},
   "source": [
    "To build our chat application, we will use a built-in LangChain chain called `ConversationalRetrievalChain`. This chain allows us to build conversational interface that is capable of retaining chat history, and perform RAG on our vector DB retriever simultaneously, without us having to code each of those steps individually. The purpose of the chat history, is when provided as a context to the model, the model will recall the conversation and may enrich the responses (with the help of additional context retrieved by the `retriever`) based on the current question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5b1ae-e472-4a29-a796-60becd9b79e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "def create_prompt_template():\n",
    "    _template = \"\"\"\n",
    "    \n",
    "Given the following chat history and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Skip the preamble and just get to the question.\n",
    "\n",
    "<chat_history>    \n",
    "{chat_history}\n",
    "</chat_history>    \n",
    "<follow_up_question>\n",
    "{question}\n",
    "</follow_up_question>\n",
    "\"\"\"\n",
    "    conversation_prompt = PromptTemplate.from_template(_template)\n",
    "    return conversation_prompt\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Skip any preamble text and reasoning and give just the answer. If the user greets you, just greet them back.\n",
    "\n",
    "<text>\n",
    "{context}\n",
    "{chat_history}\n",
    "</text>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\",\"chat_history\"])\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 3})\n",
    "\n",
    "# all_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "# windowed_memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=4, return_messages=True)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=bedrock_llm, \n",
    "                                           retriever=retriever, \n",
    "                                           condense_question_prompt=create_prompt_template(),\n",
    "                                           condense_question_llm = Bedrock(client=bedrock, model_id=\"anthropic.claude-v1\"),\n",
    "                                           combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    "                                           # verbose=True    # uncomment this to see logs\n",
    "                                          )\n",
    "\n",
    "questions = [\n",
    "    \"Hi AI, I am Bob Doe. How are you?\",\n",
    "    \"Who is the plan administrator for this plan?\",\n",
    "    \"What is the annual deductible per person?\",\n",
    "    \"Do you remember my name?\"\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\":chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer'].strip()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd5768-1705-4d16-ad86-e1ea342db36f",
   "metadata": {},
   "source": [
    "We just had an automated chat session with a bunch of pre-determined questions and we also noticed that from the fourth question, the model is able to answer the name since we have access to the chat history. Keep in mind, as the chat session goes longer, the chat memory can get bigger and bigger (i.e. commented line `all_memory` uses ALL chat history). In such cases, it is important to limit how far you want to remember the chat so that you don't run out of token limits, and encounter slower responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df716d-5ad2-45d0-96e8-9ab29eef790b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a804e26-2859-4381-9042-1b4eeb054a8a",
   "metadata": {},
   "source": [
    "## The Chat App with Gradio\n",
    "---\n",
    "\n",
    "Next we will build a simple chat app using Gradio and the same method we used above using `ConversationalRetrievalChain` and our vector database as a retriever. Note that our vector database is currently loaded with only one document. But you can imagine that you could have any number of documents loaded into the vector database.\n",
    "\n",
    "Once you run the following code cell, here are some questions you can ask in the chat interface-\n",
    "\n",
    "- Hi I am John Doe.\n",
    "- Who is the plan Administrator?\n",
    "- Who are the third party administrator?\n",
    "- What is the per-person deductible?\n",
    "- What is ERISA?\n",
    "- Do you remember my name?       ---> Test if the bot remembers your name\n",
    "- Based on your previous answers, who are the primary and the third party administrators of the plan? ---> Test chat history\n",
    "- What is Co-pay?\n",
    "- What is a deductible?\n",
    "- What is the co-pay maximum for a family?  ---> Info not in the document\n",
    "- What is the co-pay maximum for a person?  ---> Info not in the document\n",
    "- What is the deductible maximum for a family?\n",
    "- what is the maximum out of pocket for pharmacy for a person?\n",
    "- what is the maximum out of pocket for pharmacy for a family?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b1979-df5c-4f50-9d07-8e3c38d3bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "def create_prompt_template():\n",
    "    _template = \"\"\"\n",
    "    \n",
    "Given the following chat history and a follow up question, simply respond back with the question without any modifications. Skip any preamble text and reasoning and just generate the question.\n",
    "\n",
    "<chat_history>\n",
    "{chat_history}\n",
    "</chat_history>\n",
    "<follow_up_question>\n",
    "{question}\n",
    "</follow_up_question>\n",
    "\"\"\"\n",
    "    conversation_prompt = PromptTemplate.from_template(_template)\n",
    "    return conversation_prompt\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Always respond in the language the question was asked. Skip any preamble text and reasoning and give just the answer. If the user greets you, just greet them back.\n",
    "\n",
    "<text>\n",
    "{context}\n",
    "{chat_history}\n",
    "</text>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\",\"chat_history\"])\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 3})\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=bedrock_llm, \n",
    "                                           retriever=retriever, \n",
    "                                           condense_question_prompt=create_prompt_template(),\n",
    "                                           condense_question_llm = Bedrock(client=bedrock, model_id=\"anthropic.claude-v1\"),\n",
    "                                           combine_docs_chain_kwargs={\"prompt\": qa_prompt}\n",
    "                                          )\n",
    "chat_history = []\n",
    "\n",
    "def qa_fn(message, history):\n",
    "    result = qa({\"question\": message, \"chat_history\":chat_history})\n",
    "    chat_history.append((message, result[\"answer\"]))\n",
    "    return result['answer'].strip()\n",
    "\n",
    "gr.ChatInterface(qa_fn).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239af7b-2181-4492-9572-4d2541f6a54b",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "\n",
    "Let's clean up the file we uploaded to S3 earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb84d02-0b63-4cb4-bfbe-ca54046db668",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3api delete-object --bucket {data_bucket} --key bedrock-sample/health_plan.pdf"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
